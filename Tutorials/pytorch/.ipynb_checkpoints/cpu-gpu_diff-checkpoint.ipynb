{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3904,
     "status": "ok",
     "timestamp": 1606070896733,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "1IGDnRzHR4hA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3596,
     "status": "ok",
     "timestamp": 1606070896735,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "W-dP5uYXR-9k",
    "outputId": "51fa9691-baff-45d4-c95e-45cf0a28655e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3062,
     "status": "ok",
     "timestamp": 1606070896737,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "Cpx6qHpFSL2E",
    "outputId": "dd3dbe43-0f15-47a3-d2d9-eaffd2ee3143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 22 18:48:16 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10746,
     "status": "ok",
     "timestamp": 1606070994109,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "ppTxWAxmSCNw",
    "outputId": "e2c342ad-1ac7-4cbf-822c-c9ec9f6a803e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  9,  8],\n",
      "        [ 7,  8, 10]], device='cuda:0')\n",
      "tensor([[ 8.,  9.,  8.],\n",
      "        [ 7.,  8., 10.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(2,10,size=(2,3))\n",
    "y = torch.randint(2,10,size=(2,3))\n",
    "\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C1HDWvZTciA"
   },
   "source": [
    "## From CPU to GPU and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1606071170252,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "e5jIezb-SNe-",
    "outputId": "45003651-d034-499c-8526-840865dabf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n"
     ]
    }
   ],
   "source": [
    "# create a tensor\n",
    "x = torch.rand(3,2)\n",
    "# copy to GPU\n",
    "y = x.cuda()\n",
    "# copy back to CPU\n",
    "z = y.cpu()\n",
    "# get CPU tensor as numpy array\n",
    "# cannot get GPU tensor as numpy array directly\n",
    "try:\n",
    "    y.numpy()\n",
    "    #z.numpy()\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHkBE7uSTi9q"
   },
   "source": [
    "## Cannot mix CPU and GPU operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2R0az31TDw3"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(3,5)  # CPU tensor\n",
    "y = torch.rand(5,4).cuda()  # GPU tensor\n",
    "try:\n",
    "    torch.mm(x,y)  # Operation between CPU and GPU fails\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU memory is quite limited. You will frequently run into the following error:\n",
    "• RuntimeError: CUDA out of memory. Tried to allocate 12.50 MiB (GPU 0; 10.92 GiB total\n",
    "capacity; 8.57 MiB already allocated; 9.28 GiB free; 4.68 MiB cached)\n",
    "• When this happens, either reduce the batch size or check if there are any dangling\n",
    "unused tensors left on the GPU. You can delete tensors on the GPU and free memory\n",
    "with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll be running into Cuda errors like: RuntimeError: CUDA error: device-side assert triggered\n",
    "-\n",
    "-  This can mean many things. For example:\n",
    "-  You did an operation between CPU and GPU tensors\n",
    "-  You did GPU operations between tensors of unexpected shape\n",
    "-  Likely the most common cause\n",
    "-  Your types were wrong in some weird way\n",
    "-  Long when it expects a Float or vice versa is most common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9nkjWEMUAXo"
   },
   "source": [
    "### Put tensor on CUDA if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1606071388474,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "vfuThQbfTMfJ",
    "outputId": "8744608e-85d2-4bb8-d482-19e3766b6a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3347, 0.5490],\n",
      "        [0.5036, 0.1003],\n",
      "        [0.8164, 0.8855]], device='cuda:0') torch.float32\n",
      "tensor([[0.1121, 0.3014],\n",
      "        [0.2536, 0.0101],\n",
      "        [0.6665, 0.7841]], device='cuda:0')\n",
      "tensor([[0.1121, 0.3014],\n",
      "        [0.2536, 0.0101],\n",
      "        [0.6665, 0.7841]]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand(3,2)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    print(x, x.dtype)\n",
    "    \n",
    "# Do some calculations\n",
    "y = x ** 2 \n",
    "print(y)\n",
    "\n",
    "# Copy to CPU if on GPU\n",
    "if y.is_cuda:\n",
    "    y = y.cpu()\n",
    "    print(y, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFlthRSVUZaZ"
   },
   "source": [
    "### Convinient way to Create Tensor on corresponding Device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1606071746608,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "k64RZ3znUIcS",
    "outputId": "e75b2af4-35a5-4f44-aa20-fcaa9eb39dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.7353,  0.0000]])\n",
      "tensor([[0.1121, 0.3014]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(3,2)\n",
    "x2 = x1.new(1,2)  # create cpu tensor\n",
    "print(x2)\n",
    "x1 = torch.rand(3,2).cuda()\n",
    "x2 = x1.new(1,2)  # create cuda tensor\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbnsmqPGVpVo"
   },
   "source": [
    "Calculations executed on the GPU can be many times faster than numpy. However, numpy is still optimized for the CPU and many times faster than python for loops. Numpy calculations may be faster than GPU calculations for small arrays due to the cost of interfacing with the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_v-1SxEaa6n"
   },
   "source": [
    "# Differentiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1443,
     "status": "ok",
     "timestamp": 1606072062420,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "a2m01HYKVf54",
    "outputId": "dadabab0-ab6e-4d3f-aa4a-fa4f1a038168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 983.0039690000376ms\n",
      "GPU: 115.76144300011038ms\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "# Create random data\n",
    "x = torch.rand(1000,64)\n",
    "y = torch.rand(64,32)\n",
    "number = 10000  # number of iterations\n",
    "\n",
    "def square():\n",
    "    z=torch.mm(x, y) # dot product (mm=matrix multiplication)\n",
    "\n",
    "# Time CPU\n",
    "print('CPU: {}ms'.format(timeit(square, number=number)*1000))\n",
    "# Time GPU\n",
    "x, y = x.cuda(), y.cuda()\n",
    "print('GPU: {}ms'.format(timeit(square, number=number)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obRFrHnXXKzf"
   },
   "source": [
    "Link : https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "1. If you set its attribute *.requires_grad* as True, it starts to track all operations on it. \n",
    "2. When you finish your computation you can call *.backward()* and have all the gradients computed automatically\n",
    "3. The gradient for this tensor will be accumulated into *.grad* attribute\n",
    "4. To stop a tensor from tracking history, you can call *.detach()* to detach it from the computation history, and to prevent future computation from being tracked.\n",
    "5. To prevent tracking history (and using memory), you can also wrap the code block in *with torch.no_grad():*. This can be particularly helpful when evaluating a model because the model may have trainable parameters with requires_grad=True, but for which we don’t need the gradients.\n",
    "5. Each tensor has a *.grad_fn* attribute that references a Function that has created the Tensor\n",
    "6. If you want to compute the derivatives, you can call *.backward()* on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to *backward()*, however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1606072710783,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "1TVZJx7EY4ZM",
    "outputId": "12578868-a237-4770-e8ce-1747cf7f7b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo4uG0ivZdnC"
   },
   "source": [
    "## Print gradients d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1606072809848,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "yOeHZaY2Y60B",
    "outputId": "38592e04-cc9f-4024-f805-fcf6931e7e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMtMhgLpYYFc"
   },
   "source": [
    "You should have got a matrix of $4.5 .$ Let's call the out Tensor \" ${ }_{0} \"$. We have that $o=\\frac{1}{4} \\Sigma_{i} z_{i}, z_{i}=3\\left(x_{i}+2\\right)^{2}$ and $\\left.z_{i}\\right|_{x_{i}=1}=27 .$ Therefore, $\\frac{\\partial o}{\\partial x_{i}}=\\frac{3}{2}\\left(x_{i}+2\\right),$ hence $\\left.\\frac{\\partial o}{\\partial x_{i}}\\right|_{x_{i}=1}=\\frac{9}{2}=4.5 .$\n",
    "Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
    "$$\n",
    "J=\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "Generally speaking, torch.autograd is an engine for computing vector-Jacobian product. That is, given any vector\n",
    "$v=\\left(v_{1} \\quad v_{2} \\quad \\cdots v_{m}\\right)^{T},$ compute the product ${ }_{v}{ }^{T} \\cdot J .$ If ${ }_{v}$ happens to be the gradient of a scalar function $l=g(\\vec{y}),$ that is, $v=\\left(\\frac{\\partial l}{\\partial y_{1}} \\ldots \\frac{\\partial l}{\\partial y_{m}}\\right)^{T},$ then by the chain rule, the vector-Jacobian product would be the gradient of ${ }_{l}$ with respect to $\\vec{x}$ :\n",
    "$$\n",
    "J^{T} \\cdot v=\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\frac{\\partial l}{\\partial y_{1}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial y_{m}}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "\\frac{\\partial l}{\\partial x_{1}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial x_{n}}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "(Note that $v^{T} \\cdot J$ gives a row vector which can be treated as a column vector by taking $\\left.{ J}^{T} \\cdot v .\\right)$\n",
    "This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWLK4qbhahTg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ug5g5pBrlOQg"
   },
   "source": [
    "## Here the function is not scalar. I.e in above example,\n",
    "## f(x) = scalar , here f(x) = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1606075813110,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "n_OTPUBEk_4A",
    "outputId": "4f8cd13c-db84-4963-caf7-464e8a57735c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 818.5206, 1263.0530, -894.5714], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1606075825875,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "IO_7VStSk_8g",
    "outputId": "3205a542-596f-4c85-9843-3e219ad3fdcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRWYvZDglIR7"
   },
   "source": [
    "## with torch.no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1606075840515,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "KiCklYghlHIG",
    "outputId": "26708d6a-da51-4254-d0cd-bafc9aad9f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-mBJzxzgELp"
   },
   "source": [
    "### To Differentiate a vector Add it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1606074384864,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "FQHw5GJXWkRJ",
    "outputId": "adb0db3a-4532-446a-f788-6d3fa54eb9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
      "tensor([0., 2., 4., 6.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Create differentiable tensor\n",
    "x = torch.tensor(torch.arange(0,4,dtype = torch.float), requires_grad=True)\n",
    "print(x.dtype)\n",
    "# Calculate y=sum(x**2)\n",
    "y = x**2\n",
    "# Calculate gradient (dy/dx=2x)\n",
    "y.sum().backward()\n",
    "# Print values\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGk1enN-h_lW"
   },
   "source": [
    "### Gradient gets accumulated: So,Zero it out first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1606074967283,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "5KT4dlrYaqwn",
    "outputId": "7b633868-1d1a-4c39-ec7a-60e171bcaa13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([0., 2., 4., 6.], dtype=torch.float64)\n",
      "tensor([ 0.,  4.,  8., 12.], dtype=torch.float64)\n",
      "tensor([0., 2., 4., 6.], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Create a variable\n",
    "x=torch.tensor(torch.arange(0,4,dtype=float), requires_grad=True)\n",
    "print(x)\n",
    "# Differentiate\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)\n",
    "# Differentiate again (accumulates gradient)\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)\n",
    "# Zero gradient before differentiating\n",
    "x.grad.data.zero_()\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshAYB1zjCZK"
   },
   "source": [
    "## *Tensor with gradfn cannot be converted to numpy*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "error",
     "timestamp": 1606075343421,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "HTkxqVNJgIkT",
    "outputId": "018fe73b-e5d1-4b7c-83ad-bb4d1b221040"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8c8213f31fb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# raises an exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
    "x.numpy() # raises an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Llkm3KR6jUoE"
   },
   "source": [
    "The reason is that pytorch remembers the graph of all computations to perform differenciation. To be integrated to this graph the raw data is wrapped internally to the Tensor class (like what was formerly a Variable). You can detach the tensor from the graph using the .detach() method, which returns a tensor with the same data but requires_grad set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1606075461129,
     "user": {
      "displayName": "Bibek Karki",
      "photoUrl": "",
      "userId": "06703367306645254252"
     },
     "user_tz": 360
    },
    "id": "LUd-EaaJjOCG",
    "outputId": "a5fa15f7-b845-4769-c7dc-e36e7a021bce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., 16., 81.])"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor(torch.arange(0,4,dtype=float), requires_grad=True)\n",
    "y=x**2\n",
    "z=y**2\n",
    "z.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJVwgFXjj9dx"
   },
   "source": [
    "Another reason to use this method is that updating the graph can use a lot of memory. If you are in a context where you have a differentiable tensor that you don't need to differentiate, think of detaching it from the graph."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqOgA7nY3FkA5+3wjdnSzZ",
   "name": "cpu-gpu_diff.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
