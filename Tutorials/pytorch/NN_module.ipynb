{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dpzxZe8KnarJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K75es34aoh-J"
   },
   "source": [
    "### Simple Perceptron\n",
    "[<img align=\"left\"  width=\"720px\" src=\"https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp\" />]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUKLrrACn88P",
    "outputId": "5e6c1f8e-147d-4ff4-ceca-2fd992756be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  8.1955,   1.0579,  15.0859,  -6.3511, -20.9711,   1.7192,   8.9273,\n",
      "         14.1909,  -9.6321,  -1.2501], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,32).float()\n",
    "net = torch.nn.Linear(32,10)#Perceptron\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT8Kb-SVtg59"
   },
   "source": [
    "## High level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h-Dv9sxarjNB"
   },
   "outputs": [],
   "source": [
    "# create a simple sequential network (`nn.Module` object) from layers (other `nn.Module` objects).\n",
    "# Here a MLP with 2 layers and sigmoid activation.\n",
    "net =nn.Sequential(\n",
    "    nn.Linear(32,128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(128,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6mem1d5uJkz"
   },
   "source": [
    "## Equivalent: Customized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y0tkfbd-tqR2"
   },
   "outputs": [],
   "source": [
    "# create a more customizable network module (equivalent here)\n",
    "class MyNetwork(nn.Module):\n",
    "    # you can use the layer sizes as initialization arguments if you want to\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_size,hidden_size)\n",
    "        self.layer2 = torch.nn.Sigmoid()\n",
    "        self.layer3 = torch.nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, input_val):\n",
    "        h = input_val\n",
    "        h = self.layer1(h)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        return h\n",
    "\n",
    "net = MyNetwork(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cczmiZXXvxPf"
   },
   "source": [
    "The network tracks parameters, and you can access them through the parameters() method, which returns a python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBUGi2sSurPw",
    "outputId": "577c472f-5b63-4435-91ae-15fe19d3d7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0445, -0.0993,  0.0723,  ..., -0.1723, -0.0934, -0.0818],\n",
      "        [-0.1584,  0.0480, -0.0258,  ..., -0.1634, -0.0084,  0.1414],\n",
      "        [ 0.0462,  0.1074,  0.0803,  ..., -0.0347,  0.0871,  0.0650],\n",
      "        ...,\n",
      "        [ 0.1212,  0.0222,  0.0637,  ...,  0.0033, -0.1068,  0.1179],\n",
      "        [ 0.1629,  0.0222,  0.0655,  ...,  0.1437,  0.0393,  0.0208],\n",
      "        [ 0.0641, -0.1387,  0.0744,  ...,  0.0038,  0.0276, -0.0968]],\n",
      "       requires_grad=True)\n",
      "torch.Size([128, 32]) \n",
      "\n",
      " ==============================\n",
      "Parameter containing:\n",
      "tensor([-0.0248, -0.1726, -0.1525, -0.0972, -0.0193, -0.0769, -0.0084, -0.0597,\n",
      "         0.1447,  0.0664, -0.0304,  0.0216, -0.1537,  0.0302, -0.1199,  0.1206,\n",
      "        -0.0987, -0.0624, -0.0368,  0.0228, -0.1737, -0.1218,  0.1544,  0.0848,\n",
      "        -0.1738,  0.0609, -0.0617, -0.1352,  0.1411,  0.1150,  0.0559,  0.1451,\n",
      "        -0.0823, -0.0339,  0.0223,  0.1260, -0.1543, -0.1100,  0.0125, -0.0470,\n",
      "         0.0596, -0.0356,  0.1399, -0.0292, -0.1046,  0.0335, -0.1106,  0.1112,\n",
      "        -0.0859, -0.0545, -0.1410,  0.1164, -0.1380, -0.1306,  0.0447,  0.1533,\n",
      "        -0.1012,  0.0392, -0.1556,  0.1741, -0.0245, -0.1442,  0.0949, -0.0064,\n",
      "         0.0064, -0.0983, -0.1401,  0.0997,  0.1092,  0.1131, -0.0303,  0.1123,\n",
      "         0.1178, -0.1260, -0.1670, -0.0009, -0.1637,  0.0150,  0.1503,  0.1050,\n",
      "        -0.1645,  0.0817,  0.0049, -0.0960, -0.1015, -0.1200,  0.1626, -0.0687,\n",
      "         0.1046, -0.0307,  0.1084, -0.1625, -0.1289, -0.1328, -0.0915,  0.0615,\n",
      "        -0.1299, -0.0628,  0.0817, -0.0123,  0.1686, -0.0804, -0.1241,  0.1108,\n",
      "        -0.0463,  0.1341,  0.1104,  0.0822, -0.0989, -0.1312,  0.0720, -0.0572,\n",
      "         0.1635,  0.1512, -0.1072, -0.0548,  0.1330, -0.1749, -0.0694,  0.0083,\n",
      "         0.1667, -0.1390,  0.1298, -0.1542, -0.1170, -0.0392, -0.0505,  0.0441],\n",
      "       requires_grad=True)\n",
      "torch.Size([128]) \n",
      "\n",
      " ==============================\n",
      "Parameter containing:\n",
      "tensor([[ 0.0738,  0.0305, -0.0612,  ..., -0.0272,  0.0611,  0.0085],\n",
      "        [ 0.0472, -0.0012,  0.0166,  ...,  0.0114, -0.0504,  0.0800],\n",
      "        [-0.0704,  0.0377, -0.0695,  ..., -0.0675, -0.0154, -0.0249],\n",
      "        ...,\n",
      "        [ 0.0708,  0.0812, -0.0807,  ..., -0.0476, -0.0743,  0.0867],\n",
      "        [-0.0752, -0.0522, -0.0543,  ..., -0.0612, -0.0439, -0.0802],\n",
      "        [-0.0089, -0.0323,  0.0579,  ..., -0.0701,  0.0213, -0.0609]],\n",
      "       requires_grad=True)\n",
      "torch.Size([10, 128]) \n",
      "\n",
      " ==============================\n",
      "Parameter containing:\n",
      "tensor([-0.0771,  0.0381, -0.0319, -0.0242, -0.0196,  0.0852,  0.0033,  0.0019,\n",
      "        -0.0808,  0.0575], requires_grad=True)\n",
      "torch.Size([10]) \n",
      "\n",
      " ==============================\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)\n",
    "    print(param.shape,'\\n\\n','='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0bUQpFTwmnf"
   },
   "source": [
    "Parameters are of type Parameter, which is basically a wrapper for a tensor. How does pytorch retrieve your network's parameters ? They are simply all the attributes of type Parameter in your network. Moreover, if an attribute is of type nn.Module, its own parameters are added to your network's parameters ! This is why, when you define a network by adding up basic components such as nn.Linear, you should never have to explicitely define parameters.\n",
    "\n",
    "However, if you are in a case where no pytorch default module does what you need, you can define parameters explicitely (this should be rare). For the record, let's build the previous MLP with personnalized parameters.\n",
    "<br>\n",
    "<br>\n",
    "Parameters are useful in that they are meant to be all the network's weights that will be optimized during training. If you were needing to use a tensor in your computational graph that you want to remain constant, just define it as a regular tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4nvd8GqwqBN"
   },
   "source": [
    "## From Scratch: MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dt9eSZJRv0DQ"
   },
   "outputs": [],
   "source": [
    "class MyNetworkWithParams(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super(MyNetworkWithParams,self).__init__()\n",
    "        self.layer1_weights = nn.Parameter(torch.randn(input_size,hidden_size))\n",
    "        self.layer1_bias = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.layer2_weights = nn.Parameter(torch.randn(hidden_size,output_size))\n",
    "        self.layer2_bias = nn.Parameter(torch.randn(output_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h1 = torch.matmul(x,self.layer1_weights) + self.layer1_bias\n",
    "        h1_act = torch.max(h1, torch.zeros(h1.size())) # ReLU\n",
    "        output = torch.matmul(h1_act,self.layer2_weights) + self.layer2_bias\n",
    "        return output\n",
    "\n",
    "net = MyNetworkWithParams(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIzLvU1QxO4r"
   },
   "source": [
    "# **Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RNp5KoAgwxd7"
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(32,128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(128,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uljLHKfq0BGc",
    "outputId": "317d4582-66ea-4079-a559-f29b1c264084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n",
      "tensor(2.3651, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.arange(32), np.zeros(32),np.ones(32)]).float()\n",
    "y = torch.tensor([0,3,9])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output = net(x)\n",
    "print(output.shape)\n",
    "loss = criterion(output,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eXCazVt0ox-"
   },
   "source": [
    "nn.CrossEntropyLoss does both the softmax and the actual cross-entropy : given $output$ of size $(n,d)$ and $y$ of size $n$ and values in $0,1,...,d-1$, it computes $\\sum_{i=0}^{n-1}log(s[i,y[i]])$ where $s[i,j] = \\frac{e^{output[i,j]}}{\\sum_{j'=0}^{d-1}e^{output[i,j']}}$\n",
    "\n",
    "You can also compose nn.LogSoftmax and nn.NLLLoss to get the same result. Note that all these use the log-softmax rather than the softmax, for stability in the computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyPvs16P2Bsa"
   },
   "source": [
    "## LogSum trick for Softmax\n",
    "\\begin{aligned}\n",
    "\\log \\left(\\frac{e^{x_{j}}}{\\sum_{i=1}^{n} e^{x_{i}}}\\right) &=\\log \\left(e^{x_{j}}\\right)-\\log \\left(\\sum_{i=1}^{n} e^{x_{i}}\\right) \\\\\n",
    "&=x_{j}-\\log \\left(\\sum_{i=1}^{n} e^{x_{i}}\\right)\n",
    "\\end{aligned}\n",
    "---\n",
    "\\begin{aligned}\n",
    "\\log \\operatorname{Sum} \\operatorname{Exp}\\left(x_{1} \\ldots x_{n}\\right) &=\\log \\left(\\sum_{i=1}^{n} e^{x_{i}}\\right) \\\\\n",
    "&=\\log \\left(\\sum_{i=1}^{n} e^{x_{i}-c} e^{c}\\right) \\\\\n",
    "&=\\log \\left(e^{c} \\sum_{i=1}^{n} e^{x_{i}-c}\\right) \\\\\n",
    "&=\\log \\left(\\sum_{i=1}^{n} e^{x_{i}-c}\\right)+\\log \\left(e^{c}\\right) \\\\\n",
    "&=\\log \\left(\\sum_{i=1}^{n} e^{x_{i}-c}\\right)+c\n",
    "\\end{aligned}\n",
    "---\n",
    "\\begin{aligned}\n",
    "\\log \\left(\\operatorname{Softmax}\\left(x_{j}, x_{1} \\ldots x_{n}\\right)\\right) &=x_{j}-\\log \\operatorname{Sum} \\operatorname{Exp}\\left(x_{1} \\ldots x_{n}\\right) \\\\\n",
    "&=x_{j}-\\log \\left(\\sum_{i=1}^{n} e^{x_{i}-c}\\right)-c\n",
    "\\end{aligned}\n",
    "---\n",
    "Choice of c: $max(x_{1}.... x_{n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW1ec3ZT0YqP",
    "outputId": "2f1d489d-9427-4ad9-9f6a-c3676996983b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3651, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent\n",
    "criterion2 = nn.NLLLoss()\n",
    "sf = nn.LogSoftmax()\n",
    "output = net(x)\n",
    "loss = criterion(sf(output),y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzIoK-HE9xLg"
   },
   "source": [
    "Now, to perform the backward pass, just execute loss.backward() ! It will update gradients in all differentiable tensors in the graph, which in particular includes all the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-GHcCUg9ecd",
    "outputId": "82455281-3a03-449a-933f-c9801ddb970c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0015, -0.0015, -0.0015,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038],\n",
      "        [ 0.0003, -0.0033, -0.0069,  ..., -0.1045, -0.1081, -0.1117],\n",
      "        ...,\n",
      "        [ 0.0052,  0.0052,  0.0052,  ...,  0.0052,  0.0052,  0.0052],\n",
      "        [-0.0033, -0.0033, -0.0033,  ..., -0.0033, -0.0033, -0.0033],\n",
      "        [ 0.0028,  0.0031,  0.0035,  ...,  0.0127,  0.0131,  0.0134]])\n",
      "tensor([-4.5962e-03,  6.6439e-03, -2.7030e-03, -1.2382e-02,  8.0240e-04,\n",
      "         6.4603e-03,  8.7266e-03,  3.0661e-03, -2.4326e-03, -1.1182e-04,\n",
      "         4.1626e-03, -3.0078e-05, -3.1039e-03, -9.1784e-03, -7.4236e-03,\n",
      "         5.0126e-03, -3.1953e-03, -1.2337e-03, -8.7900e-03, -3.3571e-03,\n",
      "         5.6152e-03,  4.8836e-03,  6.8169e-03,  6.4579e-03,  5.8289e-03,\n",
      "        -5.0017e-03,  1.5008e-03, -3.3783e-03,  4.6842e-03, -7.7432e-04,\n",
      "        -4.2872e-03,  1.6882e-04,  8.5564e-04, -9.7265e-03, -9.2429e-03,\n",
      "        -6.4451e-03,  4.6472e-03, -5.6689e-03,  8.3494e-04,  2.6616e-03,\n",
      "        -1.6049e-03,  1.1990e-02,  1.5635e-03, -1.3143e-04, -2.7318e-03,\n",
      "        -8.7221e-03,  8.0943e-03, -1.1467e-03,  2.7352e-03, -7.0297e-03,\n",
      "         6.1524e-03, -7.2240e-04, -7.2437e-04, -6.3992e-03, -7.3744e-03,\n",
      "         3.9571e-03,  4.0865e-04,  4.3867e-03,  2.0720e-03, -3.9957e-03,\n",
      "        -7.5544e-03, -6.0805e-03,  4.5371e-03,  3.3180e-05,  4.9718e-03,\n",
      "         4.5292e-03, -4.9046e-03,  4.1835e-03, -3.0932e-03, -3.0811e-03,\n",
      "         1.5305e-03, -2.3058e-03, -9.7338e-04,  1.3094e-02, -2.9648e-03,\n",
      "        -8.5432e-04,  3.5142e-03, -4.2564e-03, -8.2810e-03, -1.0707e-02,\n",
      "        -1.8330e-03, -1.6904e-04,  5.3491e-04, -9.3849e-04,  9.7013e-03,\n",
      "         3.8219e-03,  9.7691e-03,  3.7107e-03, -1.1790e-04,  3.0223e-03,\n",
      "         1.0566e-02, -6.3493e-03,  2.1639e-03,  3.7401e-03, -5.0208e-04,\n",
      "         4.5750e-03, -4.7867e-03,  3.8544e-03, -5.3873e-03,  6.0588e-03,\n",
      "         5.2154e-03, -1.0706e-04, -1.2865e-02, -9.1264e-03,  4.6956e-03,\n",
      "        -3.9228e-04,  4.3165e-03, -6.8644e-04, -1.2979e-02, -1.4573e-03,\n",
      "        -6.1998e-03, -1.5269e-03, -2.8986e-03,  7.7581e-03, -4.6328e-03,\n",
      "        -1.8382e-03,  1.2432e-04, -7.7785e-03,  1.1604e-04,  1.0687e-02,\n",
      "         1.6074e-03,  1.0477e-02,  2.4203e-03, -2.5445e-03, -7.1814e-03,\n",
      "         7.9780e-03, -3.0610e-03,  4.7221e-03])\n",
      "tensor([[-0.2966, -0.2953, -0.2305,  ..., -0.2940,  0.0162, -0.2915],\n",
      "        [ 0.0799,  0.0832,  0.0694,  ...,  0.0861,  0.0384,  0.0795],\n",
      "        [ 0.0666,  0.0689,  0.0570,  ...,  0.0710,  0.0272,  0.0662],\n",
      "        ...,\n",
      "        [ 0.0611,  0.0633,  0.0527,  ...,  0.0653,  0.0274,  0.0609],\n",
      "        [ 0.0539,  0.0557,  0.0460,  ...,  0.0573,  0.0213,  0.0535],\n",
      "        [-0.0823, -0.1008, -0.0800,  ..., -0.1130, -0.1016, -0.0709]])\n",
      "tensor([-0.2803,  0.1186,  0.0940, -0.2518,  0.0756,  0.1006,  0.1630,  0.0887,\n",
      "         0.0754, -0.1838])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "# Check that the parameters now have gradients\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFYToquC91bE",
    "outputId": "474351bd-3c2e-4b97-dcda-9cac738c1a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0029, -0.0029, -0.0029,  ..., -0.0028, -0.0028, -0.0028],\n",
      "        [ 0.0076,  0.0076,  0.0076,  ...,  0.0076,  0.0076,  0.0076],\n",
      "        [ 0.0007, -0.0066, -0.0138,  ..., -0.2090, -0.2162, -0.2235],\n",
      "        ...,\n",
      "        [ 0.0105,  0.0105,  0.0105,  ...,  0.0105,  0.0105,  0.0105],\n",
      "        [-0.0067, -0.0067, -0.0067,  ..., -0.0067, -0.0067, -0.0067],\n",
      "        [ 0.0056,  0.0063,  0.0070,  ...,  0.0255,  0.0262,  0.0269]])\n",
      "tensor([-9.1924e-03,  1.3288e-02, -5.4060e-03, -2.4764e-02,  1.6048e-03,\n",
      "         1.2921e-02,  1.7453e-02,  6.1321e-03, -4.8652e-03, -2.2364e-04,\n",
      "         8.3251e-03, -6.0157e-05, -6.2078e-03, -1.8357e-02, -1.4847e-02,\n",
      "         1.0025e-02, -6.3905e-03, -2.4674e-03, -1.7580e-02, -6.7142e-03,\n",
      "         1.1230e-02,  9.7673e-03,  1.3634e-02,  1.2916e-02,  1.1658e-02,\n",
      "        -1.0003e-02,  3.0016e-03, -6.7565e-03,  9.3684e-03, -1.5486e-03,\n",
      "        -8.5744e-03,  3.3764e-04,  1.7113e-03, -1.9453e-02, -1.8486e-02,\n",
      "        -1.2890e-02,  9.2944e-03, -1.1338e-02,  1.6699e-03,  5.3232e-03,\n",
      "        -3.2098e-03,  2.3981e-02,  3.1270e-03, -2.6286e-04, -5.4636e-03,\n",
      "        -1.7444e-02,  1.6189e-02, -2.2934e-03,  5.4704e-03, -1.4059e-02,\n",
      "         1.2305e-02, -1.4448e-03, -1.4487e-03, -1.2798e-02, -1.4749e-02,\n",
      "         7.9141e-03,  8.1730e-04,  8.7734e-03,  4.1440e-03, -7.9915e-03,\n",
      "        -1.5109e-02, -1.2161e-02,  9.0743e-03,  6.6360e-05,  9.9435e-03,\n",
      "         9.0585e-03, -9.8092e-03,  8.3670e-03, -6.1865e-03, -6.1621e-03,\n",
      "         3.0609e-03, -4.6115e-03, -1.9468e-03,  2.6189e-02, -5.9297e-03,\n",
      "        -1.7086e-03,  7.0283e-03, -8.5129e-03, -1.6562e-02, -2.1414e-02,\n",
      "        -3.6660e-03, -3.3807e-04,  1.0698e-03, -1.8770e-03,  1.9403e-02,\n",
      "         7.6439e-03,  1.9538e-02,  7.4214e-03, -2.3580e-04,  6.0446e-03,\n",
      "         2.1132e-02, -1.2699e-02,  4.3279e-03,  7.4803e-03, -1.0042e-03,\n",
      "         9.1500e-03, -9.5733e-03,  7.7088e-03, -1.0775e-02,  1.2118e-02,\n",
      "         1.0431e-02, -2.1411e-04, -2.5730e-02, -1.8253e-02,  9.3912e-03,\n",
      "        -7.8456e-04,  8.6331e-03, -1.3729e-03, -2.5958e-02, -2.9146e-03,\n",
      "        -1.2400e-02, -3.0538e-03, -5.7972e-03,  1.5516e-02, -9.2656e-03,\n",
      "        -3.6763e-03,  2.4865e-04, -1.5557e-02,  2.3209e-04,  2.1373e-02,\n",
      "         3.2148e-03,  2.0955e-02,  4.8406e-03, -5.0890e-03, -1.4363e-02,\n",
      "         1.5956e-02, -6.1221e-03,  9.4442e-03])\n",
      "tensor([[-0.5932, -0.5906, -0.4610,  ..., -0.5881,  0.0324, -0.5829],\n",
      "        [ 0.1598,  0.1663,  0.1387,  ...,  0.1723,  0.0767,  0.1590],\n",
      "        [ 0.1332,  0.1378,  0.1140,  ...,  0.1420,  0.0544,  0.1323],\n",
      "        ...,\n",
      "        [ 0.1222,  0.1266,  0.1054,  ...,  0.1307,  0.0549,  0.1218],\n",
      "        [ 0.1077,  0.1113,  0.0920,  ...,  0.1146,  0.0427,  0.1070],\n",
      "        [-0.1645, -0.2016, -0.1601,  ..., -0.2260, -0.2033, -0.1419]])\n",
      "tensor([-0.5606,  0.2372,  0.1881, -0.5037,  0.1512,  0.2011,  0.3261,  0.1775,\n",
      "         0.1508, -0.3676])\n"
     ]
    }
   ],
   "source": [
    "# if I forward prop and backward prop again, gradients accumulate :\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUvylBl0-FKb",
    "outputId": "65a25830-7577-49e6-f0c5-dcc3bfada559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0015, -0.0015, -0.0015,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        [ 0.0037,  0.0037,  0.0037,  ...,  0.0037,  0.0037,  0.0037],\n",
      "        [ 0.0003, -0.0026, -0.0055,  ..., -0.0840, -0.0870, -0.0899],\n",
      "        ...,\n",
      "        [ 0.0050,  0.0050,  0.0050,  ...,  0.0050,  0.0050,  0.0050],\n",
      "        [-0.0034, -0.0034, -0.0034,  ..., -0.0034, -0.0034, -0.0034],\n",
      "        [ 0.0027,  0.0030,  0.0033,  ...,  0.0123,  0.0126,  0.0130]])\n",
      "tensor([-4.6727e-03,  6.3220e-03, -2.0580e-03, -1.2365e-02,  6.6032e-04,\n",
      "         6.1108e-03,  8.4204e-03,  2.8265e-03, -2.5810e-03, -2.3217e-04,\n",
      "         3.7667e-03, -1.6370e-04, -3.2447e-03, -9.1137e-03, -7.3391e-03,\n",
      "         4.9606e-03, -3.3576e-03, -1.4155e-03, -8.7772e-03, -3.4888e-03,\n",
      "         5.3576e-03,  4.5596e-03,  6.4182e-03,  5.8286e-03,  5.4092e-03,\n",
      "        -4.8970e-03,  1.4591e-03, -3.5905e-03,  4.5060e-03, -1.0049e-03,\n",
      "        -4.2782e-03,  4.3726e-07,  4.8739e-04, -9.5770e-03, -9.1103e-03,\n",
      "        -6.4961e-03,  4.3143e-03, -5.8625e-03,  7.4166e-04,  2.4487e-03,\n",
      "        -1.8786e-03,  1.1450e-02,  1.4019e-03, -3.4328e-04, -2.8370e-03,\n",
      "        -8.6785e-03,  8.3573e-03, -1.3676e-03,  2.3678e-03, -7.2665e-03,\n",
      "         5.9397e-03, -7.6929e-04, -9.5939e-04, -6.4702e-03, -7.2946e-03,\n",
      "         3.6637e-03,  2.3657e-04,  4.0229e-03,  1.8850e-03, -4.0017e-03,\n",
      "        -7.4869e-03, -6.1492e-03,  4.1655e-03, -2.3747e-04,  4.5454e-03,\n",
      "         3.6332e-03, -5.0101e-03,  3.8521e-03, -3.2468e-03, -3.2329e-03,\n",
      "         1.3070e-03, -2.5081e-03, -1.1463e-03,  1.3179e-02, -3.0967e-03,\n",
      "        -8.2623e-04,  3.4112e-03, -4.4486e-03, -8.1769e-03, -1.0494e-02,\n",
      "        -2.1420e-03, -4.0748e-04,  1.9333e-04, -1.1626e-03,  8.8650e-03,\n",
      "         3.5313e-03,  9.2961e-03,  3.4974e-03, -3.7372e-04,  2.7116e-03,\n",
      "         1.0037e-02, -6.3733e-03,  1.8967e-03,  3.2821e-03, -7.2810e-04,\n",
      "         4.1018e-03, -4.9374e-03,  4.3453e-03, -5.4447e-03,  5.6883e-03,\n",
      "         5.0017e-03, -2.3234e-04, -1.2694e-02, -9.0573e-03,  4.4607e-03,\n",
      "        -6.0405e-04,  4.0014e-03, -8.6932e-04, -1.4085e-02, -1.6277e-03,\n",
      "        -6.2093e-03, -1.6912e-03, -3.0727e-03,  7.4308e-03, -4.7219e-03,\n",
      "        -2.0931e-03, -1.6073e-04, -7.7269e-03,  9.9387e-05,  9.9243e-03,\n",
      "         1.2378e-03,  1.0064e-02,  1.6442e-03, -2.6694e-03, -7.2181e-03,\n",
      "         7.4621e-03, -3.2265e-03,  4.3758e-03])\n",
      "tensor([[-0.2890, -0.2875, -0.2415,  ..., -0.2861,  0.0181, -0.2837],\n",
      "        [ 0.0770,  0.0801,  0.0691,  ...,  0.0829,  0.0369,  0.0765],\n",
      "        [ 0.0640,  0.0662,  0.0570,  ...,  0.0683,  0.0264,  0.0635],\n",
      "        ...,\n",
      "        [ 0.0585,  0.0606,  0.0523,  ...,  0.0626,  0.0267,  0.0582],\n",
      "        [ 0.0518,  0.0535,  0.0460,  ...,  0.0551,  0.0208,  0.0514],\n",
      "        [-0.0736, -0.0918, -0.0711,  ..., -0.1036, -0.0980, -0.0622]])\n",
      "tensor([-0.2708,  0.1142,  0.0906, -0.2443,  0.0734,  0.0969,  0.1533,  0.0853,\n",
      "         0.0728, -0.1714])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# you can remove this behavior by reinitializing the gradients in your network parameters :\n",
    "net.zero_grad()\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbi3iVZN-RKD"
   },
   "source": [
    "We did backpropagation, but still didn't perform gradient descent. Let's define an optimizer on the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EsYupcl-H1D",
    "outputId": "92eaf5ac-b33b-4464-96a6-05f189ff3c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before gradient descent :\n",
      "Parameter containing:\n",
      "tensor([[-0.0665,  0.0787,  0.0762,  ...,  0.0451, -0.0749,  0.0715],\n",
      "        [-0.0086,  0.0721, -0.1677,  ...,  0.0910,  0.1097,  0.1713],\n",
      "        [-0.0464,  0.1281,  0.0059,  ..., -0.0247, -0.0458,  0.1560],\n",
      "        ...,\n",
      "        [ 0.1053, -0.0461, -0.1470,  ...,  0.1520, -0.1692,  0.0592],\n",
      "        [-0.0015, -0.0106,  0.0541,  ...,  0.0139, -0.1742,  0.0896],\n",
      "        [-0.1763,  0.1305, -0.0865,  ...,  0.0803,  0.1632, -0.1059]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0617, -0.0160, -0.1068,  0.0258, -0.0715, -0.0946, -0.1729,  0.1606,\n",
      "         0.1488,  0.1733,  0.0291,  0.0723,  0.0955,  0.0350,  0.0553, -0.0803,\n",
      "        -0.1007, -0.0082, -0.0058, -0.0245, -0.0759, -0.1736, -0.1110,  0.0062,\n",
      "        -0.0482,  0.0586, -0.0710,  0.0856, -0.0387, -0.1235,  0.1474,  0.0184,\n",
      "        -0.0450, -0.0980, -0.1758,  0.1344,  0.1751, -0.0039, -0.1666,  0.0593,\n",
      "         0.1385, -0.0986,  0.0507,  0.0365,  0.1493, -0.0525, -0.1604, -0.0092,\n",
      "         0.0452, -0.1671, -0.1280, -0.1657,  0.0122, -0.1719, -0.1221,  0.1294,\n",
      "         0.0369, -0.0190,  0.0889,  0.1411, -0.0086, -0.1312, -0.0415, -0.1543,\n",
      "         0.1380,  0.1458, -0.1563,  0.1503, -0.0237, -0.1433, -0.1719,  0.1372,\n",
      "         0.1035,  0.0795, -0.1726,  0.1453,  0.0202,  0.0301,  0.0641, -0.0923,\n",
      "        -0.1040, -0.0706,  0.1717,  0.1227,  0.0960, -0.1394, -0.0326, -0.1192,\n",
      "         0.1638,  0.1354,  0.0997, -0.0299,  0.0177,  0.0576, -0.1578, -0.1199,\n",
      "         0.0707, -0.0525,  0.1665, -0.0007, -0.1543,  0.0581, -0.0138, -0.0153,\n",
      "         0.0621, -0.0708, -0.0239,  0.0709, -0.0037,  0.0685, -0.1296, -0.0923,\n",
      "         0.1244, -0.1499, -0.0100,  0.0835,  0.0332,  0.0243,  0.0513,  0.0746,\n",
      "         0.1530, -0.1481, -0.0398, -0.0290,  0.1529,  0.0855,  0.0350,  0.1094],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0452,  0.0165,  0.0539,  ..., -0.0532, -0.0250, -0.0722],\n",
      "        [ 0.0317, -0.0622, -0.0334,  ..., -0.0868,  0.0657,  0.0488],\n",
      "        [ 0.0241,  0.0030,  0.0396,  ...,  0.0757, -0.0351,  0.0566],\n",
      "        ...,\n",
      "        [ 0.0808,  0.0880,  0.0274,  ...,  0.0270, -0.0661, -0.0162],\n",
      "        [-0.0390, -0.0660,  0.0400,  ...,  0.0491, -0.0624, -0.0227],\n",
      "        [ 0.0200, -0.0768, -0.0214,  ..., -0.0613,  0.0520, -0.0459]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0771,  0.0679,  0.0808, -0.0645, -0.0119, -0.0256,  0.0636, -0.0165,\n",
      "        -0.0856,  0.0742], requires_grad=True)\n",
      "Parameters after gradient descent :\n",
      "Parameter containing:\n",
      "tensor([[-0.0665,  0.0787,  0.0762,  ...,  0.0451, -0.0748,  0.0716],\n",
      "        [-0.0086,  0.0721, -0.1678,  ...,  0.0910,  0.1096,  0.1712],\n",
      "        [-0.0464,  0.1281,  0.0059,  ..., -0.0236, -0.0448,  0.1571],\n",
      "        ...,\n",
      "        [ 0.1052, -0.0461, -0.1471,  ...,  0.1520, -0.1693,  0.0592],\n",
      "        [-0.0015, -0.0106,  0.0542,  ...,  0.0140, -0.1742,  0.0896],\n",
      "        [-0.1763,  0.1305, -0.0865,  ...,  0.0801,  0.1631, -0.1060]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0617, -0.0161, -0.1068,  0.0260, -0.0715, -0.0947, -0.1730,  0.1606,\n",
      "         0.1488,  0.1733,  0.0290,  0.0723,  0.0955,  0.0351,  0.0554, -0.0804,\n",
      "        -0.1007, -0.0081, -0.0057, -0.0244, -0.0760, -0.1737, -0.1111,  0.0061,\n",
      "        -0.0483,  0.0586, -0.0710,  0.0856, -0.0388, -0.1235,  0.1475,  0.0184,\n",
      "        -0.0450, -0.0979, -0.1757,  0.1345,  0.1751, -0.0039, -0.1666,  0.0592,\n",
      "         0.1385, -0.0987,  0.0507,  0.0365,  0.1493, -0.0524, -0.1605, -0.0092,\n",
      "         0.0451, -0.1670, -0.1281, -0.1657,  0.0122, -0.1718, -0.1220,  0.1294,\n",
      "         0.0369, -0.0191,  0.0889,  0.1411, -0.0085, -0.1312, -0.0415, -0.1543,\n",
      "         0.1380,  0.1457, -0.1563,  0.1503, -0.0237, -0.1433, -0.1719,  0.1372,\n",
      "         0.1035,  0.0794, -0.1726,  0.1454,  0.0202,  0.0302,  0.0642, -0.0922,\n",
      "        -0.1040, -0.0706,  0.1717,  0.1227,  0.0959, -0.1395, -0.0327, -0.1192,\n",
      "         0.1638,  0.1354,  0.0996, -0.0299,  0.0177,  0.0576, -0.1578, -0.1199,\n",
      "         0.0708, -0.0526,  0.1666, -0.0007, -0.1544,  0.0581, -0.0137, -0.0152,\n",
      "         0.0621, -0.0708, -0.0239,  0.0709, -0.0036,  0.0685, -0.1295, -0.0923,\n",
      "         0.1245, -0.1499, -0.0100,  0.0835,  0.0332,  0.0244,  0.0513,  0.0745,\n",
      "         0.1529, -0.1482, -0.0398, -0.0290,  0.1530,  0.0854,  0.0350,  0.1094],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0422,  0.0194,  0.0562,  ..., -0.0503, -0.0252, -0.0693],\n",
      "        [ 0.0309, -0.0631, -0.0341,  ..., -0.0877,  0.0654,  0.0480],\n",
      "        [ 0.0234,  0.0023,  0.0390,  ...,  0.0750, -0.0354,  0.0559],\n",
      "        ...,\n",
      "        [ 0.0802,  0.0873,  0.0269,  ...,  0.0264, -0.0664, -0.0168],\n",
      "        [-0.0396, -0.0665,  0.0395,  ...,  0.0485, -0.0627, -0.0233],\n",
      "        [ 0.0208, -0.0758, -0.0206,  ..., -0.0602,  0.0530, -0.0452]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0743,  0.0667,  0.0798, -0.0620, -0.0126, -0.0266,  0.0619, -0.0174,\n",
      "        -0.0863,  0.0760], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Parameters before gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIF7QsHD-S8o",
    "outputId": "debe519a-5bcf-468b-d447-c0e0db6f9b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters after gradient descent :\n",
      "Parameter containing:\n",
      "tensor([[-0.0664,  0.0787,  0.0762,  ...,  0.0451, -0.0748,  0.0716],\n",
      "        [-0.0087,  0.0720, -0.1678,  ...,  0.0909,  0.1096,  0.1712],\n",
      "        [-0.0465,  0.1281,  0.0060,  ..., -0.0228, -0.0439,  0.1580],\n",
      "        ...,\n",
      "        [ 0.1052, -0.0462, -0.1471,  ...,  0.1519, -0.1693,  0.0591],\n",
      "        [-0.0014, -0.0105,  0.0542,  ...,  0.0140, -0.1742,  0.0897],\n",
      "        [-0.1764,  0.1305, -0.0866,  ...,  0.0800,  0.1630, -0.1061]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0616, -0.0161, -0.1067,  0.0261, -0.0715, -0.0948, -0.1730,  0.1606,\n",
      "         0.1488,  0.1733,  0.0290,  0.0723,  0.0955,  0.0352,  0.0555, -0.0804,\n",
      "        -0.1006, -0.0081, -0.0056, -0.0244, -0.0761, -0.1737, -0.1112,  0.0061,\n",
      "        -0.0483,  0.0587, -0.0710,  0.0857, -0.0388, -0.1235,  0.1475,  0.0184,\n",
      "        -0.0450, -0.0978, -0.1757,  0.1346,  0.1750, -0.0038, -0.1666,  0.0592,\n",
      "         0.1385, -0.0989,  0.0507,  0.0365,  0.1494, -0.0523, -0.1606, -0.0092,\n",
      "         0.0451, -0.1669, -0.1282, -0.1657,  0.0122, -0.1718, -0.1220,  0.1293,\n",
      "         0.0369, -0.0191,  0.0889,  0.1411, -0.0084, -0.1311, -0.0415, -0.1543,\n",
      "         0.1379,  0.1457, -0.1562,  0.1502, -0.0237, -0.1433, -0.1719,  0.1372,\n",
      "         0.1035,  0.0793, -0.1726,  0.1454,  0.0202,  0.0302,  0.0643, -0.0921,\n",
      "        -0.1040, -0.0706,  0.1717,  0.1227,  0.0958, -0.1395, -0.0328, -0.1193,\n",
      "         0.1638,  0.1354,  0.0995, -0.0298,  0.0177,  0.0575, -0.1578, -0.1199,\n",
      "         0.0708, -0.0526,  0.1667, -0.0008, -0.1544,  0.0581, -0.0136, -0.0151,\n",
      "         0.0621, -0.0708, -0.0240,  0.0709, -0.0035,  0.0685, -0.1295, -0.0923,\n",
      "         0.1245, -0.1500, -0.0099,  0.0835,  0.0332,  0.0244,  0.0513,  0.0744,\n",
      "         0.1529, -0.1483, -0.0399, -0.0290,  0.1530,  0.0853,  0.0351,  0.1093],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0394,  0.0223,  0.0586,  ..., -0.0474, -0.0254, -0.0665],\n",
      "        [ 0.0302, -0.0639, -0.0347,  ..., -0.0885,  0.0650,  0.0472],\n",
      "        [ 0.0228,  0.0017,  0.0384,  ...,  0.0743, -0.0356,  0.0553],\n",
      "        ...,\n",
      "        [ 0.0796,  0.0867,  0.0264,  ...,  0.0258, -0.0666, -0.0174],\n",
      "        [-0.0401, -0.0671,  0.0391,  ...,  0.0480, -0.0629, -0.0238],\n",
      "        [ 0.0216, -0.0748, -0.0199,  ..., -0.0591,  0.0540, -0.0446]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0716,  0.0655,  0.0789, -0.0596, -0.0134, -0.0276,  0.0604, -0.0182,\n",
      "        -0.0870,  0.0777], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer.step()\n",
    "\n",
    "print(\"Parameters after gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZRB5BKt-a3r",
    "outputId": "95faef3b-e05c-4ea0-d50c-ad567fe22603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0970, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9778, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8709, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7751, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6898, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6134, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5446, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4822, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4254, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3737, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3265, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2836, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2445, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2089, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1762, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1462, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1184, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0926, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0685, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0460, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0248, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0050, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9862, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9685, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9518, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9359, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9208, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9064, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8927, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8796, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8671, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8551, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8327, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8221, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7928, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7837, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7665, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7583, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7504, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7427, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7353, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7281, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7211, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7144, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7078, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6952, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6891, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6832, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6775, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6719, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6665, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6611, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6560, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6509, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6459, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6411, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6364, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6317, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6272, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6059, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6019, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5980, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5942, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5904, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5867, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5830, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5795, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5691, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5625, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5592, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5561, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5529, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5468, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5438, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5409, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5352, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5296, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5268, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5241, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5087, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5062, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4991, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4967, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4944, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4921, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4899, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4876, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4854, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4832, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4811, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4790, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4747, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4727, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4666, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4646, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4627, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4607, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4550, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4531, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4513, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4494, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4476, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4458, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4440, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4405, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4370, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4353, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4319, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4286, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4253, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4237, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4221, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4205, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4190, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4174, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4082, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4068, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4053, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3995, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3980, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3966, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3952, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3938, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3924, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3911, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3897, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3870, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3856, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3843, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3830, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3817, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3803, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3790, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3777, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3752, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3739, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3726, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3701, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3689, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3664, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3652, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3628, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3616, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3604, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3592, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3557, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3545, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3534, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3522, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3511, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3488, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3477, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3466, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3455, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3443, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3432, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3422, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3411, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3400, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3368, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3346, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3325, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3315, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3304, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3294, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3284, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3263, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3253, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3243, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3233, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3223, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3213, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3193, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3184, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3174, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3164, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3145, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3126, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3116, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3088, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3079, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3069, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3060, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3051, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3042, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3032, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2996, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2987, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2978, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2970, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2961, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2952, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2943, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2934, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2917, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2909, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2900, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2891, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2874, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2866, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2858, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2849, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2841, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2833, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2824, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2816, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2808, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2800, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2792, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2784, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2752, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2744, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2736, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2728, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2720, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2712, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2697, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2689, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2666, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2651, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2643, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2636, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2628, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2621, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2613, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2606, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2599, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2584, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2577, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2569, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2562, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2555, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2548, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2541, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2534, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2527, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2512, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2498, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2492, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2485, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2478, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2457, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2450, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2444, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2430, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2423, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2417, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2410, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2403, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2371, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2364, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2358, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2351, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2345, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2339, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2332, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2326, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2313, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2307, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2295, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2288, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2276, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2264, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2246, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2210, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2198, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2192, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2187, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2175, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2163, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2158, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2146, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2113, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2107, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2096, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2091, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2080, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2075, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2069, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2064, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2058, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2053, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2048, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2042, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2037, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2032, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2027, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2021, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1996, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1985, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1975, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1965, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1960, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1955, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1945, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1940, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1935, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1921, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1916, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1911, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1906, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1897, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1887, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1878, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1868, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1864, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1859, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1850, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1845, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1836, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1832, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1827, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1823, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1818, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1814, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1809, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1805, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1800, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1796, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1791, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1787, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1783, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1774, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1770, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1761, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1757, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1752, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1748, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1744, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1740, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1736, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1731, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1727, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1723, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1719, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1715, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1711, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1702, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1698, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1690, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1682, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1678, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1670, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1666, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1662, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1654, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1650, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1647, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1639, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1635, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1627, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1623, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1620, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1616, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1612, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1608, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1604, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1601, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1597, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1593, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1590, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1586, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1582, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1579, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1575, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1571, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1568, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1564, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1560, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1557, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1553, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1546, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1543, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1535, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1532, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1528, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1525, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1521, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1518, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1515, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1511, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1508, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1504, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1501, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1497, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1494, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1491, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1487, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1484, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1477, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1474, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1467, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1464, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1461, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1458, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1454, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1451, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1448, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1445, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1442, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1438, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1435, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1432, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1429, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1426, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1422, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1419, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1416, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1413, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1410, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1407, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1404, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1401, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1371, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1368, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1365, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1359, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1356, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1353, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1350, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1347, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1344, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1341, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1339, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1333, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1330, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1327, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1324, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1322, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1319, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1316, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1313, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1310, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1308, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1305, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1302, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1297, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1294, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1291, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1289, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1286, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1283, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1280, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1278, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1275, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1272, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1270, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1267, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1265, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1262, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1259, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1257, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1254, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1252, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1246, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1244, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1241, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1239, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1236, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1231, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1226, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1224, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1221, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1219, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1216, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1214, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1211, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1209, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1206, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1202, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1199, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1197, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1194, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1192, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1189, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1187, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1185, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1182, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1180, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1178, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1175, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1173, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1171, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1168, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1166, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1164, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1161, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1159, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1157, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1154, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1152, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1150, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1148, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1145, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1143, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1141, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1139, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1134, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1132, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1130, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1117, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1115, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1110, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1108, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1095, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1093, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1091, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1089, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1087, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1085, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1083, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1081, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1079, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1077, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1075, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1073, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1071, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1069, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1067, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1065, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1063, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1061, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1057, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1055, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1051, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1049, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1047, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1045, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1043, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1041, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1037, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1035, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1033, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1031, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1029, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1028, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1026, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1020, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1018, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1007, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1000, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0998, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0996, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0994, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0992, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0991, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0989, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0987, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0984, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0982, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0980, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0977, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0975, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0973, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0971, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0970, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0968, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0963, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0961, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0959, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0958, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0956, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0954, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0953, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0951, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0949, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0948, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0944, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0943, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0941, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0939, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0938, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0936, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0934, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0933, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0929, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0928, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0925, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0923, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0921, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0918, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0917, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0915, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0914, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0909, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0907, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0906, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0904, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0903, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0901, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0900, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0898, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0897, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0895, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0893, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0892, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0890, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0889, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0886, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0881, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0874, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0873, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0871, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0870, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0868, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0867, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0865, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0864, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0861, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0860, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0855, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0854, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0853, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0851, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0848, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0847, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0846, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0841, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0840, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0839, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0837, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0836, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0833, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0832, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0831, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0829, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0827, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0825, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0823, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0821, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0816, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0811, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0804, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0801, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0799, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0797, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0796, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0793, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0792, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0791, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0789, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0787, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0786, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0783, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0782, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0779, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0772, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0771, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0766, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0764, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0763, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0762, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0757, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0752, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0744, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0738, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0731, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0728, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0726, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0719, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0718, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0715, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0712, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0708, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0707, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0705, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0703, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0701, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0687, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0685, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0683, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0676, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0657, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0655, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0653, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0652, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0649, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0646, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0642, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0637, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# In a training loop, we should perform many GD iterations.\n",
    "n_iter = 1000\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad() # equivalent to net.zero_grad()\n",
    "    output = net(x)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuMH25Mk-l7q",
    "outputId": "61c81465-78c3-485c-93aa-721b3fca3009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.8674, -1.5083, -1.4658, -0.2442, -1.9451, -1.4340, -1.7717, -1.9070,\n",
      "         -1.4887,  4.0256],\n",
      "        [ 0.1304, -1.3213, -1.4189,  6.0413, -1.3553, -1.2953, -1.1516, -1.2912,\n",
      "         -1.4824,  3.4400],\n",
      "        [ 1.9529, -1.3238, -1.4886,  3.1046, -1.5425, -1.5455, -1.3501, -1.6006,\n",
      "         -1.5675,  5.7876]], grad_fn=<AddmmBackward>)\n",
      "Pred: tensor([0, 3, 9])\n",
      "Actual: tensor([0, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "output = net(x)\n",
    "print(output)\n",
    "\n",
    "print(f'Pred: {torch.argmax(output,axis = 1)}')\n",
    "print(f'Actual: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifbeCxMLAWh3"
   },
   "source": [
    "## Saving And Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUUJYDd8-x_i",
    "outputId": "3e052d62-57c3-4c0c-8838-2e31c174180e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])\n"
     ]
    }
   ],
   "source": [
    "# get dictionary of keys to weights using `state_dict`\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28,256),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(256,10))\n",
    "print(net.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EB2vtOOxAcRP",
    "outputId": "24171c4d-8c31-41d9-fffb-182bece6c236"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save a dictionary\n",
    "torch.save(net.state_dict(),'test.t7')\n",
    "# load a dictionary\n",
    "net.load_state_dict(torch.load('test.t7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKq1Sw1dCeRu"
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,n_hidden_layers):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.final_layer = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hidden.append(nn.Linear(128,128))\n",
    "        self.hidden = nn.ModuleList(self.hidden)## without this just python list\n",
    "                                                ## Parameter will not be absorbed to bigger Network\n",
    "                                                ## So doesnt update on optim.step()\n",
    "            \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            h = self.hidden[i](h)\n",
    "            h = self.act(h)\n",
    "        out = self.final_layer(h)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NN_module.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
